{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=TwitterDemo, master=local[2]) created by __init__ at <ipython-input-1-9e050480d6b6>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-342397df6fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[2]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TwitterDemo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mssc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 316\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=TwitterDemo, master=local[2]) created by __init__ at <ipython-input-1-9e050480d6b6>:7 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"TwitterDemo\")\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc.checkpoint( \"file:///Users/Pepe/Desktop/Github/FinalBDNR/Files/checkpoint\")\n",
    "\n",
    "socket_stream = ssc.socketTextStream(\"localhost\", 4040)\n",
    "\n",
    "lines = socket_stream.window(10)\n",
    "\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(get_stop_words('es'))         #Have around 900 stopwords\n",
    "nltk_words = list(stopwords.words('spanish'))   #Have around 150 stopwords\n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "#llamada a funci√≥n stop words\n",
    "clean(lines)\n",
    "\n",
    "\n",
    "(lines\n",
    "  .flatMap( lambda text: text.split( \" \" ) )\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") )\n",
    "  .map( lambda word: ( word.lower(), 1 ) )\n",
    "  .reduceByKey( lambda a, b: a + b )\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) )\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    "  .limit(10).registerTempTable(\"tweets\") ) )\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o27.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 159, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-1-9e050480d6b6>\", line 26, in <lambda>\n    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 58, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 689, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 384, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 355, in _inferSchema\n    first = rdd.first()\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1379, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dcee8dd2654c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 159, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-1-9e050480d6b6>\", line 26, in <lambda>\n    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 58, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 689, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 384, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\", line 355, in _inferSchema\n    first = rdd.first()\n  File \"C:\\Users\\Pepe\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1379, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "count = 0\n",
    "while count < 10:\n",
    "    time.sleep(20)\n",
    "    top_10_tweets = sqlContext.sql( 'SELECT tag, count FROM tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    sn.barplot(x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1\n",
    "\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  tag|count|\n",
      "+-----+-----+\n",
      "|    o|  133|\n",
      "|   de|  127|\n",
      "| lula|  121|\n",
      "|   do|   94|\n",
      "|    a|   88|\n",
      "|  que|   85|\n",
      "|    e|   74|\n",
      "|     |   62|\n",
      "|marco|   46|\n",
      "|  n√£o|   39|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.sql(\"SELECT * FROM tweets\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    for palabra in tweet:\n",
    "        if not palabra in stop_words:\n",
    "            output.append(palabra)\n",
    "    print(\"Tweet limpio\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
